{
  "duplicates": [
    {
      "id_a": "example-1",
      "id_b": "example-2",
      "title_a": "Sample Blog Post Title",
      "title_b": "Sample Blog Post Title",
      "slug_a": "sample-blog-post-title",
      "slug_b": "sample-blog-post-title",
      "url_a": "/blog/sample-blog-post-title",
      "url_b": "/blog/sample-blog-post-title-2",
      "similarity_score": 0.95,
      "method": "trigram",
      "reason": ">0.85 trigram similarity - titles and content very similar, likely duplicate submission"
    }
  ],
  "missing_meta": [
    {
      "id": "example-1",
      "slug": "sample-post",
      "missing": ["og_image", "og_description", "meta_description", "canonical_url"],
      "notes": "SEO fields missing; og_image not set; canonical_url missing"
    }
  ],
  "migration_sql": "-- =====================================================\n-- Blog Posts Metadata & Deduplication Migration\n-- =====================================================\n-- Date: 2025-01-21\n-- Purpose: Add metadata columns, content hash, and deduplication support\n\n-- Step 1: Enable required extensions\nCREATE EXTENSION IF NOT EXISTS pg_trgm;\nCREATE EXTENSION IF NOT EXISTS unaccent;\n\n-- Step 2: Add new columns\nALTER TABLE blog_posts\n  ADD COLUMN IF NOT EXISTS og_title VARCHAR(255),\n  ADD COLUMN IF NOT EXISTS og_description TEXT,\n  ADD COLUMN IF NOT EXISTS og_image VARCHAR(500),\n  ADD COLUMN IF NOT EXISTS meta_description TEXT,\n  ADD COLUMN IF NOT EXISTS canonical_url VARCHAR(500),\n  ADD COLUMN IF NOT EXISTS content_hash VARCHAR(64),\n  ADD COLUMN IF NOT EXISTS is_archived BOOLEAN DEFAULT false,\n  ADD COLUMN IF NOT EXISTS similarity_score FLOAT,\n  ADD COLUMN IF NOT EXISTS normalized_content TEXT,\n  ADD COLUMN IF NOT EXISTS normalized_title TEXT,\n  ADD COLUMN IF NOT EXISTS normalized_excerpt TEXT;\n\n-- Step 3: Create function to normalize text\nCREATE OR REPLACE FUNCTION normalize_blog_text(input_text TEXT)\nRETURNS TEXT AS $$\nBEGIN\n  IF input_text IS NULL THEN\n    RETURN '';\n  END IF;\n  \n  -- Remove HTML tags using regex\n  input_text := regexp_replace(input_text, '<[^>]+>', '', 'g');\n  \n  -- Decode common HTML entities\n  input_text := replace(input_text, '&amp;', '&');\n  input_text := replace(input_text, '&lt;', '<');\n  input_text := replace(input_text, '&gt;', '>');\n  input_text := replace(input_text, '&quot;', '\"');\n  input_text := replace(input_text, '&#39;', ''');\n  input_text := replace(input_text, '&nbsp;', ' ');\n  \n  -- Convert to lowercase\n  input_text := lower(input_text);\n  \n  -- Remove punctuation (keep alphanumeric and spaces)\n  input_text := regexp_replace(input_text, '[^a-z0-9\\s]', '', 'g');\n  \n  -- Collapse multiple whitespace to single space\n  input_text := regexp_replace(input_text, '\\s+', ' ', 'g');\n  \n  -- Trim\n  input_text := trim(input_text);\n  \n  RETURN input_text;\nEND;\n$$ LANGUAGE plpgsql IMMUTABLE;\n\n-- Step 4: Create function to compute content hash\nCREATE OR REPLACE FUNCTION compute_content_hash(\n  p_title TEXT,\n  p_excerpt TEXT,\n  p_content TEXT\n)\nRETURNS VARCHAR(64) AS $$\nDECLARE\n  normalized_title TEXT;\n  normalized_excerpt TEXT;\n  normalized_content TEXT;\n  combined TEXT;\nBEGIN\n  normalized_title := normalize_blog_text(p_title);\n  normalized_excerpt := normalize_blog_text(p_excerpt);\n  normalized_content := normalize_blog_text(p_content);\n  \n  combined := normalized_title || '|' || normalized_excerpt || '|' || normalized_content;\n  \n  RETURN encode(digest(combined, 'sha256'), 'hex');\nEND;\n$$ LANGUAGE plpgsql IMMUTABLE;\n\n-- Step 5: Populate normalized columns and content_hash for existing posts\nUPDATE blog_posts\nSET\n  normalized_title = normalize_blog_text(title),\n  normalized_excerpt = normalize_blog_text(excerpt),\n  normalized_content = normalize_blog_text(content),\n  content_hash = compute_content_hash(title, excerpt, content)\nWHERE content_hash IS NULL;\n\n-- Step 6: Create indexes\nCREATE INDEX IF NOT EXISTS idx_blog_posts_content_hash ON blog_posts(content_hash);\nCREATE INDEX IF NOT EXISTS idx_blog_posts_slug ON blog_posts(slug);\nCREATE INDEX IF NOT EXISTS idx_blog_posts_status ON blog_posts(status);\nCREATE INDEX IF NOT EXISTS idx_blog_posts_is_archived ON blog_posts(is_archived) WHERE is_archived = true;\n\n-- GIN index for full-text search\nCREATE INDEX IF NOT EXISTS idx_blog_posts_fts ON blog_posts\n  USING gin(to_tsvector('english', \n    coalesce(title, '') || ' ' || \n    coalesce(excerpt, '') || ' ' || \n    coalesce(content, '')\n  ));\n\n-- GIN trigram index for similarity searches\nCREATE INDEX IF NOT EXISTS idx_blog_posts_trgm_content ON blog_posts\n  USING gin(normalized_content gin_trgm_ops);\n\nCREATE INDEX IF NOT EXISTS idx_blog_posts_trgm_title ON blog_posts\n  USING gin(normalized_title gin_trgm_ops);\n\n-- Step 7: Add unique constraint on slug (if site_id exists, use composite)\n-- Note: Adjust based on your actual schema\nDO $$\nBEGIN\n  IF EXISTS (SELECT 1 FROM information_schema.columns \n             WHERE table_name = 'blog_posts' AND column_name = 'site_id') THEN\n    CREATE UNIQUE INDEX IF NOT EXISTS ux_blog_posts_site_slug \n      ON blog_posts(site_id, slug) \n      WHERE is_archived = false;\n  ELSE\n    CREATE UNIQUE INDEX IF NOT EXISTS ux_blog_posts_slug \n      ON blog_posts(slug) \n      WHERE is_archived = false;\n  END IF;\nEXCEPTION\n  WHEN duplicate_table THEN\n    NULL;\nEND $$;\n\n-- Step 8: Create audit log table for deduplication actions\nCREATE TABLE IF NOT EXISTS blog_dedup_actions (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  post_id UUID NOT NULL REFERENCES blog_posts(id) ON DELETE CASCADE,\n  action VARCHAR(50) NOT NULL, -- 'archived', 'merged', 'redirected', 'flagged'\n  reason TEXT,\n  resolved_by UUID REFERENCES auth.users(id),\n  resolved_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),\n  metadata JSONB,\n  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()\n);\n\nCREATE INDEX IF NOT EXISTS idx_blog_dedup_actions_post_id ON blog_dedup_actions(post_id);\nCREATE INDEX IF NOT EXISTS idx_blog_dedup_actions_resolved_at ON blog_dedup_actions(resolved_at);\n\n-- Step 9: Create trigger to auto-update content_hash on insert/update\nCREATE OR REPLACE FUNCTION update_blog_post_hash()\nRETURNS TRIGGER AS $$\nBEGIN\n  NEW.normalized_title := normalize_blog_text(NEW.title);\n  NEW.normalized_excerpt := normalize_blog_text(NEW.excerpt);\n  NEW.normalized_content := normalize_blog_text(NEW.content);\n  NEW.content_hash := compute_content_hash(NEW.title, NEW.excerpt, NEW.content);\n  RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nDROP TRIGGER IF EXISTS trigger_update_blog_post_hash ON blog_posts;\nCREATE TRIGGER trigger_update_blog_post_hash\n  BEFORE INSERT OR UPDATE OF title, excerpt, content ON blog_posts\n  FOR EACH ROW\n  EXECUTE FUNCTION update_blog_post_hash();\n\n-- Step 10: Create function to find exact duplicates\nCREATE OR REPLACE FUNCTION find_exact_duplicates()\nRETURNS TABLE (\n  content_hash VARCHAR(64),\n  post_ids UUID[],\n  count BIGINT,\n  titles TEXT[]\n) AS $$\nBEGIN\n  RETURN QUERY\n  SELECT\n    bp.content_hash,\n    array_agg(bp.id ORDER BY bp.created_at) AS post_ids,\n    count(*) AS count,\n    array_agg(bp.title ORDER BY bp.created_at) AS titles\n  FROM blog_posts bp\n  WHERE bp.content_hash IS NOT NULL\n    AND bp.is_archived = false\n  GROUP BY bp.content_hash\n  HAVING count(*) > 1\n  ORDER BY count DESC;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Step 11: Create function to find near-duplicates\nCREATE OR REPLACE FUNCTION find_near_duplicates(\n  similarity_threshold FLOAT DEFAULT 0.85,\n  limit_results INT DEFAULT 200\n)\nRETURNS TABLE (\n  id_a UUID,\n  id_b UUID,\n  title_a TEXT,\n  title_b TEXT,\n  slug_a TEXT,\n  slug_b TEXT,\n  similarity_score FLOAT\n) AS $$\nBEGIN\n  RETURN QUERY\n  SELECT\n    p1.id AS id_a,\n    p2.id AS id_b,\n    p1.title AS title_a,\n    p2.title AS title_b,\n    p1.slug AS slug_a,\n    p2.slug AS slug_b,\n    similarity(p1.normalized_content, p2.normalized_content) AS similarity_score\n  FROM blog_posts p1\n  JOIN blog_posts p2 ON p1.id < p2.id\n  WHERE p1.is_archived = false\n    AND p2.is_archived = false\n    AND p1.normalized_content IS NOT NULL\n    AND p2.normalized_content IS NOT NULL\n    AND similarity(p1.normalized_content, p2.normalized_content) > similarity_threshold\n  ORDER BY similarity_score DESC\n  LIMIT limit_results;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Step 12: Comments\nCOMMENT ON COLUMN blog_posts.content_hash IS 'SHA256 hash of normalized title|excerpt|content for duplicate detection';\nCOMMENT ON COLUMN blog_posts.normalized_content IS 'Normalized content (HTML stripped, lowercased, punctuation removed) for similarity matching';\nCOMMENT ON COLUMN blog_posts.og_title IS 'Open Graph title (max 60 chars recommended)';\nCOMMENT ON COLUMN blog_posts.og_description IS 'Open Graph description (max 160 chars recommended)';\nCOMMENT ON COLUMN blog_posts.og_image IS 'Open Graph image URL';\nCOMMENT ON COLUMN blog_posts.meta_description IS 'Meta description for SEO (max 160 chars recommended)';\nCOMMENT ON COLUMN blog_posts.canonical_url IS 'Canonical URL to prevent duplicate content issues';\n",
  "dedup_script": "-- =====================================================\n-- Blog Posts Deduplication Script\n-- =====================================================\n-- Run this script to identify and resolve duplicates\n-- Output: JSON report for CI/CD integration\n\n-- Step 1: Find exact duplicates (same content_hash)\nWITH exact_dupes AS (\n  SELECT\n    content_hash,\n    array_agg(id ORDER BY created_at) AS post_ids,\n    array_agg(title ORDER BY created_at) AS titles,\n    array_agg(slug ORDER BY created_at) AS slugs,\n    count(*) AS dup_count,\n    min(created_at) AS earliest_created\n  FROM blog_posts\n  WHERE content_hash IS NOT NULL\n    AND is_archived = false\n  GROUP BY content_hash\n  HAVING count(*) > 1\n)\nSELECT\n  json_agg(\n    json_build_object(\n      'content_hash', content_hash,\n      'post_ids', post_ids,\n      'titles', titles,\n      'slugs', slugs,\n      'count', dup_count,\n      'earliest_post_id', post_ids[1],\n      'action', 'archive_duplicates'\n    )\n  ) AS exact_duplicates\nFROM exact_dupes;\n\n-- Step 2: Find near-duplicates using trigram similarity\nWITH near_dupes AS (\n  SELECT\n    p1.id AS id_a,\n    p2.id AS id_b,\n    p1.title AS title_a,\n    p2.title AS title_b,\n    p1.slug AS slug_a,\n    p2.slug AS slug_b,\n    similarity(p1.normalized_content, p2.normalized_content) AS sim_score\n  FROM blog_posts p1\n  JOIN blog_posts p2 ON p1.id < p2.id\n  WHERE p1.is_archived = false\n    AND p2.is_archived = false\n    AND p1.normalized_content IS NOT NULL\n    AND p2.normalized_content IS NOT NULL\n    AND similarity(p1.normalized_content, p2.normalized_content) >= 0.85\n  ORDER BY sim_score DESC\n  LIMIT 200\n)\nSELECT\n  json_agg(\n    json_build_object(\n      'id_a', id_a,\n      'id_b', id_b,\n      'title_a', title_a,\n      'title_b', title_b,\n      'slug_a', slug_a,\n      'slug_b', slug_b,\n      'similarity_score', sim_score,\n      'method', 'trigram',\n      'action', CASE\n        WHEN sim_score >= 0.95 THEN 'flag_for_review'\n        WHEN sim_score >= 0.90 THEN 'suggest_merge'\n        ELSE 'manual_review'\n      END\n    )\n  ) AS near_duplicates\nFROM near_dupes;\n\n-- Step 3: Auto-archive exact duplicates (keep earliest)\n-- WARNING: Review before running!\n/*\nDO $$\nDECLARE\n  dup_record RECORD;\n  keep_id UUID;\n  archive_ids UUID[];\nBEGIN\n  FOR dup_record IN\n    SELECT\n      content_hash,\n      array_agg(id ORDER BY created_at) AS post_ids,\n      min(created_at) AS earliest\n    FROM blog_posts\n    WHERE content_hash IS NOT NULL\n      AND is_archived = false\n    GROUP BY content_hash\n    HAVING count(*) > 1\n  LOOP\n    keep_id := dup_record.post_ids[1];\n    archive_ids := dup_record.post_ids[2:array_length(dup_record.post_ids, 1)];\n    \n    -- Archive duplicates\n    UPDATE blog_posts\n    SET is_archived = true,\n        updated_at = NOW()\n    WHERE id = ANY(archive_ids);\n    \n    -- Log action\n    INSERT INTO blog_dedup_actions (post_id, action, reason, metadata)\n    SELECT\n      id,\n      'archived',\n      'Exact duplicate of post ' || keep_id,\n      jsonb_build_object('kept_post_id', keep_id, 'content_hash', dup_record.content_hash)\n    FROM unnest(archive_ids) AS id;\n    \n    RAISE NOTICE 'Archived % duplicates, kept post %', array_length(archive_ids, 1), keep_id;\n  END LOOP;\nEND $$;\n*/\n\n-- =====================================================\n-- Python/Node.js Pseudocode for CI Integration\n-- =====================================================\n\n/*\nPSEUDOCODE (Python):\n\nimport hashlib\nimport re\nfrom typing import List, Dict\n\ndef normalize_text(text: str) -> str:\n    \"\"\"Normalize text for hashing and similarity checks\"\"\"\n    if not text:\n        return ''\n    \n    # Remove HTML tags\n    text = re.sub(r'<[^>]+>', '', text)\n    \n    # Decode HTML entities\n    html_entities = {\n        '&amp;': '&',\n        '&lt;': '<',\n        '&gt;': '>',\n        '&quot;': '\"',\n        '&#39;': \"'\",\n        '&nbsp;': ' '\n    }\n    for entity, char in html_entities.items():\n        text = text.replace(entity, char)\n    \n    # Lowercase\n    text = text.lower()\n    \n    # Remove punctuation\n    text = re.sub(r'[^a-z0-9\\s]', '', text)\n    \n    # Collapse whitespace\n    text = re.sub(r'\\s+', ' ', text)\n    \n    return text.strip()\n\ndef compute_content_hash(title: str, excerpt: str, content: str) -> str:\n    \"\"\"Compute SHA256 hash of normalized content\"\"\"\n    normalized_title = normalize_text(title)\n    normalized_excerpt = normalize_text(excerpt)\n    normalized_content = normalize_text(content)\n    \n    combined = f\"{normalized_title}|{normalized_excerpt}|{normalized_content}\"\n    \n    return hashlib.sha256(combined.encode('utf-8')).hexdigest()\n\ndef check_duplicates(db_connection, site_id: str = None) -> Dict:\n    \"\"\"Check for duplicates and return report\"\"\"\n    \n    # Find exact duplicates\n    exact_dupes_query = \"\"\"\n        SELECT content_hash, array_agg(id) as ids, count(*) as cnt\n        FROM blog_posts\n        WHERE is_archived = false\n        GROUP BY content_hash\n        HAVING count(*) > 1\n    \"\"\"\n    \n    # Find near-duplicates\n    near_dupes_query = \"\"\"\n        SELECT p1.id as id_a, p2.id as id_b,\n               similarity(p1.normalized_content, p2.normalized_content) as sim\n        FROM blog_posts p1\n        JOIN blog_posts p2 ON p1.id < p2.id\n        WHERE p1.is_archived = false AND p2.is_archived = false\n          AND similarity(p1.normalized_content, p2.normalized_content) > 0.85\n        ORDER BY sim DESC\n        LIMIT 200\n    \"\"\"\n    \n    # Execute queries and build report\n    exact_dupes = execute_query(exact_dupes_query)\n    near_dupes = execute_query(near_dupes_query)\n    \n    return {\n        'exact_duplicates': exact_dupes,\n        'near_duplicates': near_dupes,\n        'recommendations': generate_recommendations(exact_dupes, near_dupes)\n    }\n\ndef validate_publish(post_data: Dict) -> Dict:\n    \"\"\"Validate post before publishing\"\"\"\n    errors = []\n    warnings = []\n    \n    # Check required fields\n    if not post_data.get('title'):\n        errors.append('Title is required')\n    \n    if not post_data.get('content'):\n        errors.append('Content is required')\n    \n    # Check slug uniqueness\n    slug_check = check_slug_uniqueness(post_data['slug'], post_data.get('site_id'))\n    if slug_check['exists']:\n        errors.append(f\"Slug '{post_data['slug']}' already exists\")\n    \n    # Check for near-duplicates\n    content_hash = compute_content_hash(\n        post_data.get('title', ''),\n        post_data.get('excerpt', ''),\n        post_data.get('content', '')\n    )\n    \n    similar_posts = find_similar_posts(content_hash, threshold=0.85)\n    if similar_posts:\n        warnings.append(f\"Found {len(similar_posts)} similar posts\")\n    \n    # Validate metadata\n    if not post_data.get('og_title'):\n        warnings.append('OG title missing, will use title')\n    \n    if not post_data.get('og_description'):\n        warnings.append('OG description missing, will use excerpt')\n    \n    if not post_data.get('meta_description'):\n        warnings.append('Meta description missing, will use excerpt')\n    \n    return {\n        'valid': len(errors) == 0,\n        'errors': errors,\n        'warnings': warnings\n    }\n\n# CI/CD Integration Example\nif __name__ == '__main__':\n    # Run on every publish\n    report = check_duplicates(db_connection)\n    \n    if report['exact_duplicates']:\n        print('ERROR: Found exact duplicates!')\n        exit(1)\n    \n    if len(report['near_duplicates']) > 10:\n        print('WARNING: Found many near-duplicates')\n        # Continue but flag for review\n    \n    # Save report to file\n    save_json_report(report, 'duplicate_report.json')\n*/\n",
  "checklist": [
    "Compute content_hash on save/update using trigger",
    "Require unique(site_id, slug) where is_archived = false",
    "Enforce minimum content length: 2000 characters (HTML stripped)",
    "Run similarity check against last 500 posts before publish (threshold: 0.85)",
    "Block publish if exact duplicate (same content_hash) exists and is not archived",
    "Block publish if content length < 2000 characters (after HTML stripping)",
    "Auto-generate og_title from title (trim to 60 chars) if missing",
    "Auto-generate og_description from excerpt (trim to 160 chars) if missing",
    "Auto-generate meta_description from excerpt (trim to 160 chars) if missing",
    "Set default og_image if missing (use site default or post featured_image)",
    "Set canonical_url to post URL if missing",
    "Validate tags: ensure comma-separated, lowercased, unique, count >= 1",
    "Check slug format: lowercase, alphanumeric + hyphens only",
    "Verify excerpt length: max 300 chars recommended",
    "Verify title length: 50-60 chars for optimal SEO",
    "Run full-text search index update after publish",
    "Log all publish actions to audit table",
    "Send notification if similarity_score > 0.90 for manual review"
  ],
  "recommendations": [
    "Enforce minimum content length of 2000 characters (after HTML stripping) at database level",
    "Show content length counter in blog editor UI (e.g., '1,234 / 2,000 characters')",
    "Block publish/submit if content < 2000 characters with clear error message",
    "Auto-generate og_description from excerpt up to 160 chars if missing",
    "If og_image missing, use default OG image per site (configure in site settings)",
    "If og_image missing and featured_image exists, use featured_image as og_image",
    "Set canonical_url automatically to: https://yoursite.com/blog/{slug}",
    "For exact duplicates: keep earliest post by created_at, archive others automatically",
    "For near-duplicates (sim >= 0.95): auto-archive if same author and created within 7 days",
    "For near-duplicates (sim 0.85-0.95): flag for manual review, don't auto-archive",
    "Maintain redirect mapping for archived duplicate posts to canonical version",
    "Add content_hash to API responses for client-side duplicate detection",
    "Implement scheduled job to run deduplication check weekly",
    "Create admin dashboard to review and resolve flagged duplicates",
    "Add similarity_score to post preview in admin panel",
    "Consider using vector embeddings (pgvector) for more advanced similarity detection",
    "Add 'duplicate_of' foreign key column to link archived posts to canonical version",
    "Implement soft-delete pattern: is_archived instead of hard delete for audit trail"
  ]
}

